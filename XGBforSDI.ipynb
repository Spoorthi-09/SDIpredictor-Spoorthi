{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "222619a6-386d-432a-80a2-795e8950fdaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math, re, json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "from typing import Optional, Dict, Tuple\n",
    "\n",
    "from sklearn.model_selection import train_test_split, KFold, cross_val_score, RandomizedSearchCV\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer, make_column_selector\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.ensemble import RandomForestRegressor, HistGradientBoostingRegressor\n",
    "from joblib import dump\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.ensemble import RandomForestRegressor, HistGradientBoostingRegressor\n",
    "\n",
    "from xgboost import XGBRegressor\n",
    "from catboost import CatBoostRegressor\n",
    "\n",
    "# ====== Project config ======\n",
    "TARGET = \"Approved Benefit Amount\"\n",
    "\n",
    "# Columns that leak post-decision info; exclude from features\n",
    "LEAKY_COLUMNS = [\n",
    "    TARGET,\n",
    "    \"Posted Date\",\n",
    "    \"Agreed Tenant Settlement\",\n",
    "    \"Agreed Tenant Settlement Date\",\n",
    "    \"Collected Date\",\n",
    "    \"Collected Amount\",\n",
    "    \"Collection Processed Date\",\n",
    "    \"Review Claim Adjudication\",\n",
    "    \"Review Tenant Information\",\n",
    "    \"Update YRIG Policy Info\",\n",
    "    \"Open Collections\",\n",
    "    \"PM Notification of Claim Received\",\n",
    "    \"Send to Collections\",\n",
    "    \"Audit Selection\",\n",
    "    \"Approval Date\",\n",
    "]\n",
    "\n",
    "CURRENCY_LIKE = [\"Max Benefit\", \"Monthly Rent\", \"Amount of Claim\", \"Approved Benefit Amount\"]\n",
    "DATE_LIKE = [\n",
    "    \"Claim Date\",\"Lease Start Date\",\"Lease End Date\",\n",
    "    \"Move-Out Date\",\"Posted Date\",\"Agreed Tenant Settlement Date\",\n",
    "    \"Collected Date\",\"Collection Processed Date\",\n",
    "]\n",
    "ZIP_COLS = [\"Lease Zip\"]\n",
    "\n",
    "# Date differences to compute (A - B) in days\n",
    "DATE_DIFFS = [\n",
    "    (\"Claim Date\", \"Move-Out Date\", \"days_claim_minus_moveout\"),\n",
    "    (\"Lease End Date\", \"Lease Start Date\", \"days_lease_duration\"),\n",
    "    (\"Move-Out Date\", \"Lease Start Date\", \"days_moveout_minus_lease_start\"),\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "54de07ff-5375-405f-b649-082cf223051a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1244, 47)\n"
     ]
    }
   ],
   "source": [
    "df_raw = pd.read_csv(\"depositClaimsData.csv\")\n",
    "print(df_raw.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c4f23a21-3de5-4f24-8a7d-12df9c1c6038",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _to_datetime(s: pd.Series) -> pd.Series:\n",
    "    return pd.to_datetime(s, errors=\"coerce\", infer_datetime_format=True)\n",
    "\n",
    "def _strip_currency(x):\n",
    "    if pd.isna(x):\n",
    "        return np.nan\n",
    "    if isinstance(x, (int, float)):\n",
    "        return float(x)\n",
    "    s = re.sub(r\"[^0-9.\\-]\", \"\", str(x))  # keep digits, dot, minus\n",
    "\n",
    "    # Guard: tokens that aren't valid numbers ('.', '-', '-.', '.-')\n",
    "    if s in {\"\", \".\", \"-\", \"-.\", \".-\"}:\n",
    "        return np.nan\n",
    "\n",
    "    # Guard: multiple dots -> keep first, drop the rest (e.g., '1.234.56' -> '1.23456')\n",
    "    if s.count(\".\") > 1:\n",
    "        parts = s.split(\".\")\n",
    "        s = parts[0] + \".\" + \"\".join(parts[1:])\n",
    "\n",
    "    try:\n",
    "        return float(s)\n",
    "    except Exception:\n",
    "        return np.nan\n",
    "\n",
    "def _first5_zip(x) -> Optional[str]:\n",
    "    if pd.isna(x): return np.nan\n",
    "    m = re.search(r\"(\\d{5})\", str(x))\n",
    "    return m.group(1) if m else np.nan\n",
    "\n",
    "def _safe_div(n, d):\n",
    "    if pd.isna(n) or pd.isna(d) or d == 0: return np.nan\n",
    "    return n / d\n",
    "\n",
    "def _date_parts(prefix: str, s: pd.Series) -> pd.DataFrame:\n",
    "    return pd.DataFrame({\n",
    "        f\"{prefix}_year\": s.dt.year,\n",
    "        f\"{prefix}_month\": s.dt.month,\n",
    "        f\"{prefix}_day\": s.dt.day,\n",
    "        f\"{prefix}_dow\": s.dt.dayofweek,\n",
    "        f\"{prefix}_is_month_start\": s.dt.is_month_start.astype(float),\n",
    "        f\"{prefix}_is_month_end\": s.dt.is_month_end.astype(float),\n",
    "    })\n",
    "\n",
    "def _yn_to_int(x):\n",
    "    xs = str(x).strip().lower()\n",
    "    if xs in [\"yes\",\"y\",\"true\",\"1\"]: return 1\n",
    "    if xs in [\"no\",\"n\",\"false\",\"0\"]: return 0\n",
    "    return np.nan\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4784c5dd-356a-4b07-8149-ca033769af4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_df(df: pd.DataFrame, target_col: str) -> Tuple[pd.DataFrame, pd.Series, Dict]:\n",
    "    df = df.copy()\n",
    "    df.columns = [c.strip() for c in df.columns]\n",
    "\n",
    "    if target_col not in df.columns:\n",
    "        raise ValueError(f\"Target '{target_col}' not found. Columns: {list(df.columns)}\")\n",
    "\n",
    "    # Currency → float\n",
    "    for c in CURRENCY_LIKE:\n",
    "        if c in df.columns:\n",
    "            df[c] = df[c].apply(_strip_currency)\n",
    "\n",
    "    # Dates → datetime\n",
    "    for c in DATE_LIKE:\n",
    "        if c in df.columns:\n",
    "            df[c] = _to_datetime(df[c])\n",
    "\n",
    "    # ZIP → first 5\n",
    "    for c in ZIP_COLS:\n",
    "        if c in df.columns:\n",
    "            df[c] = df[c].apply(_first5_zip).astype(\"object\")\n",
    "\n",
    "    # Clean objects (strip & treat empty as NaN)\n",
    "    for c in df.select_dtypes(include=\"object\").columns:\n",
    "        df[c] = df[c].astype(str).str.strip().replace({\"\": np.nan, \"nan\": np.nan})\n",
    "\n",
    "    # Date deltas\n",
    "    for a,b,name in DATE_DIFFS:\n",
    "        if a in df.columns and b in df.columns:\n",
    "            df[name] = (df[a] - df[b]).dt.days\n",
    "\n",
    "    # Calendar features (claim/moveout/lease_start)\n",
    "    if \"Claim Date\" in df.columns:\n",
    "        df = pd.concat([df, _date_parts(\"claim\", df[\"Claim Date\"])], axis=1)\n",
    "    if \"Move-Out Date\" in df.columns:\n",
    "        df = pd.concat([df, _date_parts(\"moveout\", df[\"Move-Out Date\"])], axis=1)\n",
    "    if \"Lease Start Date\" in df.columns:\n",
    "        df = pd.concat([df, _date_parts(\"lease_start\", df[\"Lease Start Date\"])], axis=1)\n",
    "\n",
    "    # Ratios\n",
    "    if {\"Amount of Claim\",\"Max Benefit\"}.issubset(df.columns):\n",
    "        df[\"claim_to_max_ratio\"] = df.apply(lambda r: _safe_div(r[\"Amount of Claim\"], r[\"Max Benefit\"]), axis=1)\n",
    "    if {\"Amount of Claim\",\"Monthly Rent\"}.issubset(df.columns):\n",
    "        df[\"claim_to_rent_ratio\"] = df.apply(lambda r: _safe_div(r[\"Amount of Claim\"], r[\"Monthly Rent\"]), axis=1)\n",
    "\n",
    "    # Tenants\n",
    "    for col in [\"Is there a 2nd Tenant?\",\"Is there a 3rd Tenant?\"]:\n",
    "        if col in df.columns:\n",
    "            df[col] = df[col].apply(_yn_to_int)\n",
    "\n",
    "    df[\"num_tenants_reported\"] = (\n",
    "        (df[\"Is there a 2nd Tenant?\"] if \"Is there a 2nd Tenant?\" in df.columns else 0).fillna(0) +\n",
    "        (df[\"Is there a 3rd Tenant?\"] if \"Is there a 3rd Tenant?\" in df.columns else 0).fillna(0) + 1\n",
    "    )\n",
    "\n",
    "    # Target\n",
    "    y = df[target_col].astype(float).copy()\n",
    "\n",
    "    # Drop leaky & explicit IDs\n",
    "    to_drop = [c for c in LEAKY_COLUMNS if c in df.columns and c != target_col]\n",
    "    for maybe_id in [\"Tracking Number\", \"Group #\", \"Treaty #\", \"Policy\"]:\n",
    "        if maybe_id in df.columns: to_drop.append(maybe_id)\n",
    "\n",
    "    df.drop(columns=to_drop + [target_col], inplace=True, errors=\"ignore\")\n",
    "\n",
    "    meta = {\"dropped_columns\": to_drop, \"target\": target_col}\n",
    "    return df, y, meta\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2975bfa6-5667-4ad2-88cd-c2f7a7f1d8ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer, make_column_selector\n",
    "from sklearn.pipeline import Pipeline\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "def build_preprocessor(X: pd.DataFrame) -> ColumnTransformer:\n",
    "    numeric_selector = make_column_selector(dtype_include=np.number)\n",
    "    categorical_selector = make_column_selector(dtype_include=object)\n",
    "\n",
    "    numeric_pipe = Pipeline(steps=[\n",
    "        (\"impute\", SimpleImputer(strategy=\"median\")),\n",
    "    ])\n",
    "\n",
    "    categorical_pipe = Pipeline(steps=[\n",
    "        (\"impute\", SimpleImputer(strategy=\"most_frequent\")),\n",
    "        (\"ohe\", OneHotEncoder(handle_unknown=\"ignore\", sparse_output=False)),\n",
    "    ])\n",
    "\n",
    "    pre = ColumnTransformer(\n",
    "        transformers=[\n",
    "            (\"num\", numeric_pipe, numeric_selector),\n",
    "            (\"cat\", categorical_pipe, categorical_selector),\n",
    "        ],\n",
    "        remainder=\"drop\",\n",
    "        verbose_feature_names_out=False,\n",
    "    )\n",
    "    return pre\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d4bb126a-1467-49a0-89a6-60a32363bf1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_508639/1672171574.py:2: UserWarning: The argument 'infer_datetime_format' is deprecated and will be removed in a future version. A strict version of it is now the default, see https://pandas.pydata.org/pdeps/0004-consistent-to-datetime-parsing.html. You can safely remove this argument.\n",
      "  return pd.to_datetime(s, errors=\"coerce\", infer_datetime_format=True)\n",
      "/tmp/ipykernel_508639/1672171574.py:2: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  return pd.to_datetime(s, errors=\"coerce\", infer_datetime_format=True)\n",
      "/tmp/ipykernel_508639/1672171574.py:2: UserWarning: The argument 'infer_datetime_format' is deprecated and will be removed in a future version. A strict version of it is now the default, see https://pandas.pydata.org/pdeps/0004-consistent-to-datetime-parsing.html. You can safely remove this argument.\n",
      "  return pd.to_datetime(s, errors=\"coerce\", infer_datetime_format=True)\n",
      "/tmp/ipykernel_508639/1672171574.py:2: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  return pd.to_datetime(s, errors=\"coerce\", infer_datetime_format=True)\n",
      "/tmp/ipykernel_508639/1672171574.py:2: UserWarning: The argument 'infer_datetime_format' is deprecated and will be removed in a future version. A strict version of it is now the default, see https://pandas.pydata.org/pdeps/0004-consistent-to-datetime-parsing.html. You can safely remove this argument.\n",
      "  return pd.to_datetime(s, errors=\"coerce\", infer_datetime_format=True)\n",
      "/tmp/ipykernel_508639/1672171574.py:2: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  return pd.to_datetime(s, errors=\"coerce\", infer_datetime_format=True)\n",
      "/tmp/ipykernel_508639/1672171574.py:2: UserWarning: The argument 'infer_datetime_format' is deprecated and will be removed in a future version. A strict version of it is now the default, see https://pandas.pydata.org/pdeps/0004-consistent-to-datetime-parsing.html. You can safely remove this argument.\n",
      "  return pd.to_datetime(s, errors=\"coerce\", infer_datetime_format=True)\n",
      "/tmp/ipykernel_508639/1672171574.py:2: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  return pd.to_datetime(s, errors=\"coerce\", infer_datetime_format=True)\n",
      "/tmp/ipykernel_508639/1672171574.py:2: UserWarning: The argument 'infer_datetime_format' is deprecated and will be removed in a future version. A strict version of it is now the default, see https://pandas.pydata.org/pdeps/0004-consistent-to-datetime-parsing.html. You can safely remove this argument.\n",
      "  return pd.to_datetime(s, errors=\"coerce\", infer_datetime_format=True)\n",
      "/tmp/ipykernel_508639/1672171574.py:2: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  return pd.to_datetime(s, errors=\"coerce\", infer_datetime_format=True)\n",
      "/tmp/ipykernel_508639/1672171574.py:2: UserWarning: The argument 'infer_datetime_format' is deprecated and will be removed in a future version. A strict version of it is now the default, see https://pandas.pydata.org/pdeps/0004-consistent-to-datetime-parsing.html. You can safely remove this argument.\n",
      "  return pd.to_datetime(s, errors=\"coerce\", infer_datetime_format=True)\n",
      "/tmp/ipykernel_508639/1672171574.py:2: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  return pd.to_datetime(s, errors=\"coerce\", infer_datetime_format=True)\n",
      "/tmp/ipykernel_508639/1672171574.py:2: UserWarning: The argument 'infer_datetime_format' is deprecated and will be removed in a future version. A strict version of it is now the default, see https://pandas.pydata.org/pdeps/0004-consistent-to-datetime-parsing.html. You can safely remove this argument.\n",
      "  return pd.to_datetime(s, errors=\"coerce\", infer_datetime_format=True)\n",
      "/tmp/ipykernel_508639/1672171574.py:2: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  return pd.to_datetime(s, errors=\"coerce\", infer_datetime_format=True)\n",
      "/tmp/ipykernel_508639/1672171574.py:2: UserWarning: The argument 'infer_datetime_format' is deprecated and will be removed in a future version. A strict version of it is now the default, see https://pandas.pydata.org/pdeps/0004-consistent-to-datetime-parsing.html. You can safely remove this argument.\n",
      "  return pd.to_datetime(s, errors=\"coerce\", infer_datetime_format=True)\n",
      "/tmp/ipykernel_508639/1672171574.py:2: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  return pd.to_datetime(s, errors=\"coerce\", infer_datetime_format=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After dropping NaN target rows: (1212, 52) (1212,)\n"
     ]
    }
   ],
   "source": [
    "# Preprocess first (handles currency/dates), then drop bad targets\n",
    "X_tmp, y_tmp, meta = preprocess_df(df_raw, TARGET)\n",
    "\n",
    "mask = y_tmp.notna() & np.isfinite(y_tmp.astype(float))\n",
    "X = X_tmp.loc[mask].reset_index(drop=True)\n",
    "y = y_tmp.loc[mask].astype(float).reset_index(drop=True)\n",
    "\n",
    "print(\"After dropping NaN target rows:\", X.shape, y.shape)\n",
    "\n",
    "# Rebuild the preprocessor on the filtered X\n",
    "pre = build_preprocessor(X)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cf92c510-5180-425a-84b4-fc7820aa700b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(969, 243)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "if len(X) >= 10:\n",
    "    test_size = 0.2\n",
    "elif len(X) >= 5:\n",
    "    test_size = 0.33\n",
    "elif len(X) >= 3:\n",
    "    test_size = 0.5\n",
    "else:\n",
    "    test_size = 0.5  \n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=test_size, random_state=42)\n",
    "len(X_train), len(X_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "703fad51-5c50-40ed-a766-241474fad044",
   "metadata": {},
   "outputs": [],
   "source": [
    "from joblib import Memory\n",
    "memory = Memory(location=\"./sk_cache\", verbose=0)  # caches preprocessing across CV folds\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# --- XGBoost on GPU\n",
    "from xgboost import XGBRegressor\n",
    "# xgb = Pipeline(\n",
    "#     [\n",
    "#         (\"prep\", pre),\n",
    "#         (\"model\", XGBRegressor(\n",
    "#             n_estimators=1000,\n",
    "#             learning_rate=0.05,\n",
    "#             max_depth=6,\n",
    "#             subsample=0.8,\n",
    "#             colsample_bytree=0.8,\n",
    "#             reg_lambda=1.0,\n",
    "#             random_state=42,\n",
    "#             device=\"cuda\",       # <-- GPU enable (v3.x)\n",
    "#             tree_method=\"hist\",  # <-- unified hist (not 'gpu_hist')\n",
    "#             predictor=\"auto\",\n",
    "#             n_jobs=1,\n",
    "#             verbosity=0,\n",
    "#         ))\n",
    "#     ],\n",
    "#     memory=memory\n",
    "# )\n",
    "xgb = Pipeline([\n",
    "    (\"prep\", pre),\n",
    "    (\"model\", XGBRegressor(\n",
    "        n_estimators=4000,\n",
    "        learning_rate=0.015,\n",
    "        max_depth=10,\n",
    "        min_child_weight=12,\n",
    "        subsample=0.8,\n",
    "        colsample_bytree=0.8,\n",
    "        colsample_bynode=0.8,\n",
    "        reg_lambda=8.0,\n",
    "        reg_alpha=0.5,\n",
    "        gamma=0.1,\n",
    "        max_bin=512,             \n",
    "        tree_method=\"hist\",\n",
    "        predictor=\"auto\",\n",
    "        n_jobs=16,\n",
    "        random_state=42,\n",
    "        verbosity=1\n",
    "    ))\n",
    "], memory=memory)\n",
    "\n",
    "\n",
    "# --- CatBoost on GPU\n",
    "from catboost import CatBoostRegressor\n",
    "cat = Pipeline(\n",
    "    [\n",
    "        (\"prep\", pre),\n",
    "        (\"model\", CatBoostRegressor(\n",
    "            depth=6,\n",
    "            learning_rate=0.05,\n",
    "            n_estimators=1000,\n",
    "            l2_leaf_reg=3.0,\n",
    "            random_state=42,\n",
    "            loss_function=\"RMSE\",\n",
    "            task_type=\"GPU\",   # << GPU training\n",
    "            devices=\"0\",\n",
    "            verbose=False,\n",
    "            allow_writing_files=False\n",
    "        ))\n",
    "    ],\n",
    "    memory=memory\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "55251c80-d14e-4f19-9189-5dc5894173d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/x1/sbhat3/.local/lib/python3.10/site-packages/xgboost/training.py:199: UserWarning: [17:31:57] WARNING: /workspace/src/learner.cc:790: \n",
      "Parameters: { \"predictor\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/x1/sbhat3/.local/lib/python3.10/site-packages/xgboost/training.py:199: UserWarning: [17:31:58] WARNING: /workspace/src/learner.cc:790: \n",
      "Parameters: { \"predictor\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/x1/sbhat3/.local/lib/python3.10/site-packages/xgboost/training.py:199: UserWarning: [17:31:58] WARNING: /workspace/src/learner.cc:790: \n",
      "Parameters: { \"predictor\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/x1/sbhat3/.local/lib/python3.10/site-packages/xgboost/training.py:199: UserWarning: [17:31:58] WARNING: /workspace/src/learner.cc:790: \n",
      "Parameters: { \"predictor\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/x1/sbhat3/.local/lib/python3.10/site-packages/xgboost/training.py:199: UserWarning: [17:31:58] WARNING: /workspace/src/learner.cc:790: \n",
      "Parameters: { \"predictor\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/usr/local/lib/python3.10/dist-packages/sklearn/impute/_base.py:597: UserWarning: Skipping features without any observed values: ['Pending Docs from PM']. At least one non-missing value is needed for imputation with strategy='most_frequent'.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/sklearn/impute/_base.py:597: UserWarning: Skipping features without any observed values: ['Pending Docs from PM']. At least one non-missing value is needed for imputation with strategy='most_frequent'.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/sklearn/impute/_base.py:597: UserWarning: Skipping features without any observed values: ['Pending Docs from PM']. At least one non-missing value is needed for imputation with strategy='most_frequent'.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/sklearn/impute/_base.py:597: UserWarning: Skipping features without any observed values: ['Pending Docs from PM']. At least one non-missing value is needed for imputation with strategy='most_frequent'.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/sklearn/impute/_base.py:597: UserWarning: Skipping features without any observed values: ['Pending Docs from PM']. At least one non-missing value is needed for imputation with strategy='most_frequent'.\n",
      "  warnings.warn(\n",
      "/x1/sbhat3/.local/lib/python3.10/site-packages/xgboost/training.py:199: UserWarning: [17:32:33] WARNING: /workspace/src/learner.cc:790: \n",
      "Parameters: { \"predictor\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/usr/local/lib/python3.10/dist-packages/sklearn/impute/_base.py:597: UserWarning: Skipping features without any observed values: ['Pending Docs from PM']. At least one non-missing value is needed for imputation with strategy='most_frequent'.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'name': 'XGB (defaults)',\n",
       "  'cv_rmse_mean': 490.77515675259053,\n",
       "  'cv_rmse_std': 35.035234405296414,\n",
       "  'val_rmse': 523.8864104810399,\n",
       "  'val_mae': 292.1122234057988,\n",
       "  'val_r2': 0.7790118660793585}]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import math, numpy as np\n",
    "from sklearn.model_selection import KFold, cross_val_score\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "\n",
    "def rmse(y_true, y_pred):\n",
    "    return math.sqrt(mean_squared_error(y_true, y_pred))\n",
    "\n",
    "def rmse_scorer(estimator, X, y):\n",
    "    pred = estimator.predict(X)\n",
    "    return -rmse(y, pred)  # sklearn maximizes scores\n",
    "\n",
    "def choose_cv_splits(n_train: int):\n",
    "    if n_train >= 5: return 5\n",
    "    if n_train >= 3: return 2\n",
    "    return None\n",
    "\n",
    "def eval_model(name, pipe, X_train, y_train, X_val, y_val):\n",
    "    n_train = len(X_train)\n",
    "    cv_k = choose_cv_splits(n_train)\n",
    "    cv_mean = cv_std = None\n",
    "    if cv_k:\n",
    "        kf = KFold(n_splits=cv_k, shuffle=True, random_state=42)\n",
    "        cvs = cross_val_score(pipe, X_train, y_train, scoring=rmse_scorer, cv=kf, n_jobs=-1, error_score='raise')\n",
    "        cv_mean, cv_std = float(np.mean(-cvs)), float(np.std(-cvs))\n",
    "    pipe.fit(X_train, y_train)\n",
    "    pred = pipe.predict(X_val)\n",
    "    return {\n",
    "        \"name\": name,\n",
    "        \"cv_rmse_mean\": cv_mean,\n",
    "        \"cv_rmse_std\": cv_std,\n",
    "        \"val_rmse\": float(rmse(y_val, pred)),\n",
    "        \"val_mae\": float(mean_absolute_error(y_val, pred)),\n",
    "        \"val_r2\": float(r2_score(y_val, pred)),\n",
    "    }\n",
    "\n",
    "results = []\n",
    "# results.append(eval_model(\"Lasso\", lasso, X_train, y_train, X_val, y_val))\n",
    "# results.append(eval_model(\"RandomForest\", rf, X_train, y_train, X_val, y_val))\n",
    "# results.append(eval_model(\"HGB (defaults)\", hgb, X_train, y_train, X_val, y_val))\n",
    "results.append(eval_model(\"XGB (defaults)\", xgb, X_train, y_train, X_val, y_val))\n",
    "# results.append(eval_model(\"CatBoost (defaults)\", cat, X_train, y_train, X_val, y_val))\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "524403db-edd2-4982-89ef-84dc6032349b",
   "metadata": {},
   "outputs": [],
   "source": [
    "##################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b3fc8a79-6553-43a2-b82e-76a0aab306cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "###########################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "094ce63b-1eff-4758-a10e-cfcf1508a687",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
